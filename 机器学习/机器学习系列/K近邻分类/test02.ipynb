{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9583333333333334\n",
      "{'n_neighbors': 9, 'weights': 'uniform'}\n",
      "KNeighborsClassifier(n_neighbors=9)\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Scikit-Learn K近邻分类\n",
    "\n",
    "# K近邻（K-Nearest Neighbors，KNN）又称最近邻，意思是K个最近的邻居，是一种有监督的学习分类器，虽然它可以用于回归问题，但它通常用作分类算法\n",
    "# KNN算法的历史最早可以追溯到1951年，当时Evelyn Fix和Joseph Hodges提出了“最近邻模型”的概念；1957年，Cover和Hart扩展了他们的概念；1992年，Altman发表的一篇名为“K-Nearest Neighbors”的文章，使得该算法真正得到了广泛的认知和应用\n",
    "# 然而，随着数据集的增长，kNN变得越来越低效，影响了模型的整体性能。KNN虽然不能像以前那么受欢迎，但由于其简单性和准确性，使得它仍然是数据科学中的首批算法之一。它通常用于简单的推荐系统、金融市场预测、数据预处理（缺失数据估计插补）等\n",
    "# K近邻假设可以在实例彼此附近找到相似点，根据比重分配类别标签，使用在给定数据点周围最多表示的标签。KNN算法的基本原理是：在特征空间中，如果一个样本最接近的K个邻居中大多数属于某一个类别，则该样本也属于这个类别。这在技术上被称为多数表决（Plurality Voting）\n",
    "# 例如，K=3，KNN算法就会找到与预测点距离最近的三个点（如图中圆圈所示），看看哪种类别多一些，就将预测点归为哪类，图示表示预测点将会被归类到Class B\n",
    "# 成功的诀窍在于如何确定数据实例之间的相似性，即怎么算是最近？因此，在进行分类之前，必须先定义距离。常见的距离度量有：欧几里得距离(p=2)、曼哈顿距离（p=1）等\n",
    "# 值得注意的是，KNN算法也是Lazy Learning模型家族的一部分，这意味着所有计算都发生在进行分类或预测时。由于它严重依赖内存来存储其所有训练数据，因此也称为基于实例或基于内存的学习方法\n",
    "\n",
    "# 超参数K\n",
    "# k-NN算法中的k值定义了将检查多少个邻居以确定查询点的分类。例如，当K=1时，实例将被分配到与其单个最近邻相同的类。K是一种平衡行为，因为不同的值可能会导致过拟合或欠拟合。较低的K值可能具有较高的方差和较低的偏差，较大的K值可能导致较高的偏差和较低的方差。K的选择将很大程度上取决于输入数据，因为有许多异常值或噪声的数据可能会在K值较高时表现更好。总之，建议K值使用奇数以避免分类歧义，交叉验证策略可以帮助我们为数据集选择最佳K值\n",
    "# 交叉验证会将样本数据按照一定比例拆分成训练数据和验证数据，然后从选取一个较小的K值开始，不断增加K的值，然后计算验证数据的误差，最终找到一个比较合适的K值\n",
    "# 一般情况下，K与模型的Validation Error（模型应用于验证数据的错误）的关系如下图所示：\n",
    "# 这个图其实很好理解，当K值增大时，一般错误率会降低，因为周围有更多的样本可以借鉴了；但需要注意的是，和K-Means不同，当K值很大时，错误率会更高，例如我们共有35个样本，当K增大到30时，数据的预测会把更多距离较远的数据也放入比较，最终导致预测偏差较大。相反，K值越小，则模型越过拟合。因此，我们需要针对特定问题选择一个最合适的K值，以保证模型的效果最佳\n",
    "\n",
    "# K近邻分类的优缺点\n",
    "# 优点：\n",
    "# - 超参数少，只需要一个K值和一个距离度量，易于理解和使用，预测效果较好\n",
    "# - 基于内存，在数据集较小时可以快速地进行训练和预测\n",
    "# - 对异常值不敏感，根据最邻近实例的类别来进行投票，从而降低了异常值对结果的影响\n",
    "# - 对数据预处理要求较低，KNN不需要对数据进行复杂的预处理，例如标准化、归一化等\n",
    "# 缺点：\n",
    "# - 在数据集较大时，需要较大的存储空间和较长的计算时间，从时间和金钱的角度来看，这可能是昂贵的\n",
    "# - 容易出现过拟合，较小的K值可能会过度拟合数据，K值太大，模型可能会欠拟合\n",
    "# - 对噪声数据敏感，如果数据集中存在噪声数据，KNN算法可能会受到较大影响\n",
    "# - 对高维数据输入表现不佳，也称峰值现象，在算法达到最佳特征数量后，额外的特征会增加分类错误的数量\n",
    "\n",
    "# KNN算法在执行时主要经历了三个阶段：\n",
    "# - 计算测试样本（待分类样本）到其他每个样本间的距离\n",
    "# - 对计算的距离进行排序，然后选择距离最小的前K个样本点\n",
    "# - 对K个样本点所属的类别进行比较，根据少数服从多数的原则，将测试样本归入在K个点中占比最高的那一类\n",
    "\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 加载数据\n",
    "\n",
    "# 加载数据集，返回类型<class 'sklearn.utils._bunch.Bunch'>\n",
    "# iris = datasets.load_iris()\n",
    "# X = iris.data\n",
    "# y = iris.target\n",
    "\n",
    "# 使用鸢尾花前两个特征的全部分类（三分类）\n",
    "# 加载数据集，返回类型<class 'sklearn.utils._bunch.Bunch'>\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "\n",
    "# 划分训练集（80%）和测试集（20%）\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# # KNN分类器（默认使用标准欧几里德度量标准，K=5）\n",
    "# knn_clf = KNeighborsClassifier()\n",
    "# # 训练\n",
    "# knn_clf.fit(X_train, y_train)\n",
    "# # 在测试集上预测\n",
    "# y_pred = knn_clf.predict(X_test)\n",
    "#\n",
    "# # 测试与标签数据的平均准确度\n",
    "# print(knn_clf.score(X_test, y_test))   # 0.6666666666666666\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def decision_boundary_fill(model, axis):\n",
    "    # np.meshgrid()：定义X/Y坐标轴上的起始点和结束点以及点的密度，返回这些网格点的X和Y坐标矩阵\n",
    "    X0, X1 = np.meshgrid(\n",
    "        np.linspace(axis[0], axis[1], int((axis[1] - axis[0]) * 100)).reshape(-1, 1),\n",
    "        np.linspace(axis[2], axis[3], int((axis[3] - axis[2]) * 100)).reshape(-1, 1)\n",
    "    )\n",
    "    # ravel()：将高维数组降为一维数组\n",
    "    # np.c_[]：将两个数组以列的形式拼接起来形成矩阵，这里将上面每个网格点的X和Y坐标组合\n",
    "    X_grid_matrix = np.c_[X0.ravel(), X1.ravel()]\n",
    "    # 通过训练好的逻辑回归模型，预测平面上这些网格点的分类\n",
    "    y_grid_pred = model.predict(X_grid_matrix)\n",
    "    y_pred_matrix = y_grid_pred.reshape(X0.shape)\n",
    "    # plt.contourf(X,Y,Z)：绘制等高线，cmap用于设置填充轮廓，默认为viridis，还可设置为热力图色彩plt.cm.hot或自定义；alpha用于设置填充透明度\n",
    "    # ListedColormap()：自定义填充色彩列表\n",
    "    custom_cmap = ListedColormap(['#EF9A9A', '#FFF59D', '#90CAF9'])\n",
    "    plt.contourf(X0, X1, y_pred_matrix, alpha=0.5, cmap=custom_cmap)\n",
    "\n",
    "# # 绘制决策边界\n",
    "# decision_boundary_fill(knn_clf, axis=[4.5, 7.0, 2, 4.5])\n",
    "# plt.scatter(X[y == 0, 0], X[y == 0, 1], color='red')\n",
    "# plt.scatter(X[y == 1, 0], X[y == 1, 1], color='blue')\n",
    "# plt.scatter(X[y == 2, 0], X[y == 2, 1], color='green')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# 交叉验证\n",
    "# Scikit-Learn内置网格搜索和交叉验证API\n",
    "# API官方文档：https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "# API中文文档：https://scikit-learn.org.cn/view/655.html\n",
    "from sklearn.model_selection import GridSearchCV    # 内置网格搜索和交叉验证\n",
    "\n",
    "# # 定义优化参数字典，字典中的Key值必须是分类器算法API中的参数名\n",
    "# # 设置K的取值（K为从1到49的奇数）；设置参数权重取值\n",
    "# params = {'n_neighbors': range(1, 50, 2), 'weights': ['uniform', 'distance']}\n",
    "# # 遍历所有取值，评估每个K值\n",
    "# grid = GridSearchCV(knn_clf, params, cv=5, scoring='accuracy')\n",
    "#\n",
    "# # 拟合所有组合\n",
    "# grid.fit(X_train, y_train)\n",
    "#\n",
    "# # 最优评分\n",
    "# print(grid.best_score_)      # 0.9583333333333334\n",
    "# # 最佳参数（最优K）\n",
    "# print(grid.best_params_)     # {'n_neighbors': 9, 'weights': 'uniform'}\n",
    "# # 获取最佳评分时的分类器模型\n",
    "# print(grid.best_estimator_)  # KNeighborsClassifier(n_neighbors=9)\n",
    "#\n",
    "#\n",
    "# # # 使用最佳参数建模并预测\n",
    "# # knn = KNeighborsClassifier(n_neighbors=9)\n",
    "# # # 训练模型\n",
    "# # knn.fit(X_train, y_train)\n",
    "# # # 预测\n",
    "# # y_pred = knn.predict(X_test)\n",
    "# # # 在测试集上的评分\n",
    "# # print(knn.score(X_test, y_test))  # 1.0\n",
    "\n",
    "\n",
    "# 使用管道\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 构建管道（使用鸢尾花前两个特征的三分类）\n",
    "clf = Pipeline(\n",
    "    steps=[(\"scaler\", StandardScaler()), (\"knn\", KNeighborsClassifier(n_neighbors=29))]\n",
    ")\n",
    "# 训练\n",
    "clf.fit(X_train, y_train)\n",
    "# 评分\n",
    "print(clf.score(X_test, y_test))   # 0.6666666666666666\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "# Scikit-Learn内置决策边界可视化API\n",
    "# API官方文档：https://scikit-learn.org/stable/modules/generated/sklearn.inspection.DecisionBoundaryDisplay.html\n",
    "def decision_boundary_disp(model, X, y):\n",
    "    disp = DecisionBoundaryDisplay.from_estimator(model, X, response_method=\"predict\", plot_method=\"pcolormesh\", alpha=0.5)\n",
    "    disp.ax_.scatter(X[:, 0], X[:, 1], c=y, edgecolors=\"k\")\n",
    "    plt.show()\n",
    "\n",
    "decision_boundary_disp(clf, X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}